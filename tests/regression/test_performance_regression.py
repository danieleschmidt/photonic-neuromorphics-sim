"""Performance regression tests to detect performance degradation."""\n\nimport pytest\nimport numpy as np\nimport time\nfrom pathlib import Path\n\n\n@pytest.mark.regression\n@pytest.mark.performance\nclass TestSimulationPerformance:\n    \"\"\"Test simulation performance to catch regressions.\"\"\"\n    \n    def test_optical_simulation_performance(\n        self, \n        mock_optical_parameters,\n        performance_benchmark\n    ):\n        \"\"\"Test optical simulation performance baseline.\"\"\"\n        # Performance baseline: 1000 wavelength points in <1 second\n        wavelengths = np.linspace(1500e-9, 1600e-9, 1000)\n        \n        with performance_benchmark as bench:\n            # Mock optical simulation\n            for wl in wavelengths[:100]:  # Reduced for test speed\n                # Simulate some computation\n                transmission = np.exp(-0.1 * (wl - 1550e-9) ** 2 / (10e-9) ** 2)\n                assert 0 <= transmission <= 1\n        \n        # Performance assertion\n        time_per_wavelength = bench.elapsed_time / 100\n        assert time_per_wavelength < 0.01  # <10ms per wavelength point\n    \n    def test_spice_simulation_performance(\n        self,\n        sample_spice_netlist,\n        performance_benchmark\n    ):\n        \"\"\"Test SPICE simulation performance baseline.\"\"\"\n        with performance_benchmark as bench:\n            # Mock SPICE simulation\n            # In real implementation, this would run actual SPICE\n            time_points = np.linspace(0, 1e-6, 1000)\n            for t in time_points[:100]:  # Reduced for test speed\n                # Simulate circuit evaluation\n                voltage = np.sin(2 * np.pi * 1e6 * t)\n                assert -1 <= voltage <= 1\n        \n        # SPICE should complete within reasonable time\n        assert bench.elapsed_time < 5.0  # <5 seconds for 100 time points\n    \n    def test_layout_generation_performance(\n        self,\n        sample_network_topology,\n        performance_benchmark\n    ):\n        \"\"\"Test layout generation performance baseline.\"\"\"\n        network_size = sample_network_topology[\"input_size\"]\n        \n        with performance_benchmark as bench:\n            # Mock layout generation\n            for i in range(min(network_size, 100)):  # Limit for test speed\n                # Simulate component placement\n                x = i * 10e-6  # 10 micron spacing\n                y = 0\n                # Mock routing calculation\n                route_length = np.sqrt(x**2 + y**2)\n                assert route_length >= 0\n        \n        # Layout generation should be fast\n        components_per_second = 100 / bench.elapsed_time\n        assert components_per_second > 10  # >10 components per second\n    \n    @pytest.mark.slow\n    def test_large_network_performance(\n        self,\n        performance_benchmark\n    ):\n        \"\"\"Test performance with large neural networks.\"\"\"\n        # Large network: 10K neurons, 1M synapses\n        num_neurons = 10000\n        num_synapses = 1000000\n        \n        with performance_benchmark as bench:\n            # Mock large network processing\n            # In real implementation, this would process actual network\n            \n            # Simulate neuron updates\n            neuron_states = np.random.rand(num_neurons)\n            \n            # Simulate synapse updates (vectorized)\n            synapse_weights = np.random.rand(num_synapses)\n            updated_weights = synapse_weights * 0.99  # Simple update\n            \n            assert len(neuron_states) == num_neurons\n            assert len(updated_weights) == num_synapses\n        \n        # Large network should still complete in reasonable time\n        assert bench.elapsed_time < 60.0  # <1 minute\n        \n        # Calculate performance metrics\n        neurons_per_second = num_neurons / bench.elapsed_time\n        synapses_per_second = num_synapses / bench.elapsed_time\n        \n        # Performance thresholds\n        assert neurons_per_second > 1000   # >1K neurons/second\n        assert synapses_per_second > 10000  # >10K synapses/second\n\n\n@pytest.mark.regression\nclass TestMemoryUsage:\n    \"\"\"Test memory usage to prevent memory leaks and excessive consumption.\"\"\"\n    \n    def test_memory_usage_stability(self):\n        \"\"\"Test that memory usage remains stable over multiple operations.\"\"\"\n        import psutil\n        import os\n        \n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss\n        \n        # Perform multiple operations\n        for i in range(100):\n            # Mock operations that might cause memory leaks\n            large_array = np.random.rand(1000, 1000)\n            result = np.sum(large_array)\n            del large_array  # Explicit cleanup\n            \n            assert result > 0\n        \n        final_memory = process.memory_info().rss\n        memory_growth = final_memory - initial_memory\n        \n        # Memory growth should be minimal (less than 100MB)\n        assert memory_growth < 100 * 1024 * 1024\n    \n    def test_no_memory_leaks_in_simulation(self):\n        \"\"\"Test for memory leaks in simulation loops.\"\"\"\n        import gc\n        \n        # Force garbage collection\n        gc.collect()\n        initial_objects = len(gc.get_objects())\n        \n        # Simulate multiple simulation runs\n        for iteration in range(10):\n            # Mock simulation that creates temporary objects\n            temp_data = {\n                f\"array_{i}\": np.random.rand(100, 100)\n                for i in range(10)\n            }\n            \n            # Process data\n            total = sum(np.sum(arr) for arr in temp_data.values())\n            assert total > 0\n            \n            # Clear references\n            temp_data.clear()\n            del temp_data\n        \n        # Force garbage collection\n        gc.collect()\n        final_objects = len(gc.get_objects())\n        \n        # Object count should not grow significantly\n        object_growth = final_objects - initial_objects\n        assert object_growth < 1000  # Less than 1000 new objects\n\n\n@pytest.mark.regression\nclass TestAccuracyRegression:\n    \"\"\"Test numerical accuracy to catch precision regressions.\"\"\"\n    \n    def test_optical_calculation_accuracy(self, mock_optical_parameters):\n        \"\"\"Test that optical calculations maintain accuracy.\"\"\"\n        # Reference values (these would be from validated simulations)\n        wavelength = mock_optical_parameters[\"wavelength\"]\n        \n        # Test transmission calculation accuracy\n        reference_transmission = 0.8542  # Reference value\n        \n        # Mock current calculation\n        current_transmission = 0.8542  # In real test, this would be calculated\n        \n        # Accuracy tolerance\n        tolerance = 1e-6\n        relative_error = abs(current_transmission - reference_transmission) / reference_transmission\n        \n        assert relative_error < tolerance\n    \n    def test_numerical_stability(self):\n        \"\"\"Test numerical stability of calculations.\"\"\"\n        # Test operations that might be numerically unstable\n        \n        # Division by small numbers\n        small_number = 1e-12\n        result = 1.0 / small_number\n        assert np.isfinite(result)\n        assert result > 0\n        \n        # Logarithms of small numbers\n        log_result = np.log(small_number)\n        assert np.isfinite(log_result)\n        \n        # Square roots of small numbers\n        sqrt_result = np.sqrt(small_number)\n        assert np.isfinite(sqrt_result)\n        assert sqrt_result > 0\n    \n    def test_parameter_sensitivity(self, mock_optical_parameters):\n        \"\"\"Test sensitivity to parameter changes.\"\"\"\n        base_params = mock_optical_parameters.copy()\n        \n        # Test small parameter variations\n        param_variations = {\n            \"wavelength\": [0.99, 1.01],  # ±1% variation\n            \"coupling_efficiency\": [0.98, 1.02]  # ±2% variation  \n        }\n        \n        for param, multipliers in param_variations.items():\n            base_value = base_params[param]\n            \n            for multiplier in multipliers:\n                varied_value = base_value * multiplier\n                \n                # Mock calculation with varied parameter\n                # In real test, this would run actual simulation\n                mock_result = varied_value / base_value  # Simple relationship\n                \n                # Result should be close to the parameter variation\n                expected_ratio = multiplier\n                actual_ratio = mock_result\n                \n                relative_error = abs(actual_ratio - expected_ratio) / expected_ratio\n                assert relative_error < 0.1  # <10% error\n\n\n@pytest.mark.benchmark\nclass TestBenchmarkSuite:\n    \"\"\"Comprehensive benchmark suite for performance tracking.\"\"\"\n    \n    def test_component_library_benchmark(self, performance_benchmark):\n        \"\"\"Benchmark component library operations.\"\"\"\n        component_types = [\"mzi\", \"microring\", \"waveguide\", \"detector\"]\n        \n        with performance_benchmark as bench:\n            for component_type in component_types:\n                # Mock component creation and basic operations\n                for i in range(100):\n                    # Simulate component instantiation\n                    params = {\"type\": component_type, \"id\": i}\n                    \n                    # Simulate parameter calculation\n                    result = sum(params.values() if isinstance(v, (int, float)) else 0 \n                               for v in params.values())\n                    assert result >= 0\n        \n        # Benchmark metrics\n        operations_per_second = (len(component_types) * 100) / bench.elapsed_time\n        assert operations_per_second > 1000  # >1K operations/second\n    \n    def test_simulation_engine_benchmark(self, performance_benchmark):\n        \"\"\"Benchmark simulation engine performance.\"\"\"\n        simulation_steps = 1000\n        \n        with performance_benchmark as bench:\n            # Mock time-stepping simulation\n            state = np.random.rand(100)  # 100-component state vector\n            \n            for step in range(simulation_steps):\n                # Mock state update\n                state = state * 0.99 + np.random.rand(100) * 0.01\n                \n                # Simple stability check\n                if step % 100 == 0:\n                    assert np.all(np.isfinite(state))\n                    assert np.all(state >= 0)\n        \n        # Performance metrics\n        steps_per_second = simulation_steps / bench.elapsed_time\n        assert steps_per_second > 100  # >100 simulation steps/second\n    \n    @pytest.mark.slow\n    def test_end_to_end_benchmark(self, performance_benchmark):\n        \"\"\"Benchmark complete end-to-end workflow.\"\"\"\n        with performance_benchmark as bench:\n            # Mock complete workflow\n            # 1. Model creation\n            model_params = {\"neurons\": 1000, \"synapses\": 10000}\n            \n            # 2. Simulation\n            for timestep in range(100):  # Reduced for test speed\n                # Mock timestep calculation\n                computation = np.random.rand(model_params[\"neurons\"])\n                result = np.sum(computation)\n                assert result > 0\n            \n            # 3. Layout generation (mock)\n            layout_elements = model_params[\"neurons\"] + model_params[\"synapses\"]\n            layout_area = layout_elements * 10e-12  # 10 µm² per element\n            \n            assert layout_area > 0\n        \n        # End-to-end performance target\n        assert bench.elapsed_time < 30.0  # <30 seconds for complete workflow\n        \n        # Log performance for tracking\n        print(f\"\\nEnd-to-end benchmark: {bench.elapsed_time:.2f} seconds\")